---
title: "Movielens recommendation project"
author: "Judy Bowen"
date: "September 14, 2021"
output:
  pdf_document: 
    latex_engine: xelatex
 
---


```{r, libraries, include=FALSE}
tinytex::install_tinytex()
library(tinytex)
library(ggrepel)
library(dplyr)
library(hms)
library(lubridate)
library(MASS)
library(forcats)
library(ggplot2)
library(tidyverse)
library(lattice)

library(caret)
library(dslabs)
library(corrplot)

library(data.table)
library(tidyselect)
library(stringr)

library(utils)
library(knitr)
library(e1071)
library(latexpdf)
```

```{r, increase-limit, include=FALSE, eval=TRUE}
memory.limit(size=6500)
knitr::opts_chunk$set(fig.width=7)
```



```{r, edx-database, eval=TRUE, include=FALSE}

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
# title = as.character(title),
# genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

#####################################################################
###################################################################
##### a review of the database for Movielens project###############
##show a few statistics about the edx database
## first up is counts by rating
##create a database that strips out movie titles and genres descriptions so that the database is smaller and more efficient
edxsmall <- edx[,c(1,2,3,4)]
edxsmall$userId <- as.factor(edxsmall$userId)

```

The purpose of this project is to use R to build a recommendation system that helps people find a movie to watch.   The data is provided as a subset of movies rated by viewers, as generated by the GroupLens research lab^[https://grouplens.org/].   The dataset for this project consists of 9,000,055 observations of six variables (userId, movieId, rating, timestamp, title and genres).  Each row of data represents a rating given by one user to one movie.  The movie recommendation system here was created using a model based upon the BellKor solution to the Netflix Grand Prize^[https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf], ^[http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/].     The BellKor solution uses a number of techniques – linear regression models, factor analysis, gradient boosted decision trees.  In this paper, I make use of the linear modelling techniques, where the regression model was defined as a loss function;  the purpose of the model is to minimize the RMSE, where y_u,i is the rating for movie i by user u, with the movie prediction defined as yhat_u,i.  The RMSE is defined as:

**sqrt(mean((true_ratings - predicted_ratings)^2)).**


In this paper the relationship between each movie by each user has the greatest impact on the model RMSE.  The model here also incorporates the time effect, as described by the BellKor solution, where the time variable is binned into 10 week sections to take into account changes in tastes over time.  Also included was a frequency variable which estimates the effect of rating a number of  movies on a single timestamp – if many movies are reviewed at one time, the BellKor group hypothesized that the mood and tastes of a user at the one time would weight more heavily on the measures of movie preferences.  Genres were also included in an attempt to relate tastes for a broader category of movies into the model.

The beta estimates obtained from the model were transformed by standardizing these estimates, as described in the BellKor paper, such that a penalty was imposed on movies with a low count of ratings.  The rating and the frequency variable were transformed, prior to running any model, by scaling these to a z-distribution.  It turned out, in my models, that scaling the ratings would yield the greatest improvement to the RMSE. Prior to scaling the ratings, I could not obtain an RMSE below 0.86, even with standardization applied to the beta estimates.  The final RMSE, obtained from the validation set, was measured at 0.8155.  


**Dataset Description and Analysis** 

The following describes the dataset in general terms (using methods as described in Data Analysis and Prediction Algorithms with R^[https://rafalab.github.io/dsbook/]).  
In general terms, the database is described as having the following unique users, movies, genres and timestamps:
```{r, unique_count, eval=TRUE, echo=FALSE, fig.cap='Count of unique variables'}
unique_count <- edx %>% summarize(n_users= n_distinct(userId), n_movies=n_distinct(movieId), n_genres=n_distinct(genres), n_timestamp=n_distinct(timestamp))
kable(unique_count, caption="Count of unique values for each variable")
```
The mean of the distribution of ratings in the edx movielens database is 3.512465.  The median is located to the right of the average rating, 4.  As the mean of the distribution is less than the median, the distribution is negatively skewed.   The measure of skewness in the edx data set is -0.5958884, also indicating that the distribution of the ratings is skewed towards the left.  
```{r, measures, echo=FALSE, eval=TRUE, out.width='80%', fig.align='left'}
###find the mean, median and skewness of the edx rating
meanedxrating <- mean(edxsmall$rating)
medianedxrating <- median(edxsmall$rating)
skewnessedsrating <- skewness(edxsmall$rating)
measures <- tibble(measure=c("mean", "median", "skewness"), value=c(meanedxrating, medianedxrating, skewnessedsrating) )
kable(measures, caption="Mean, median and skewness measures of the rating variable")
```

The distribution of the average rating is illustrated in the histograms, below for average rating by movieId and then average rating by userId (for userId which have 100 or more ratings).  The average rating by movieId and the median rating by movieId  also illustrate evidence of negative skew in the dataset:

```{r , averages-plots, echo=FALSE, eval=TRUE, fig.dim=c(7,4)}
##look at the distribution of the average movie Rating
avgmovieratingmovieId <- edxsmall %>% group_by(movieId) %>% summarise(avg_rating = mean(rating))
avgmovieratinguserId <- edxsmall %>% group_by(userId) %>% filter(n()>=100) %>% summarise(avg_rating=mean(rating))
distribavgratingmovieId <- ggplot(avgmovieratingmovieId, aes(x=avg_rating))+geom_histogram(bins=50)+geom_vline(data=avgmovieratingmovieId, aes(xintercept=mean(avg_rating)), col="red")+geom_vline(data=avgmovieratingmovieId, aes(xintercept=median(avg_rating)), col="blue")+ggtitle("distribution of average rating by movieId; blue line= median, red line= average")+theme(plot.title=element_text(size=10))
distribavgratinguserId <- ggplot(avgmovieratinguserId, aes(x=avg_rating))+geom_histogram(bins=50)+geom_vline(data=avgmovieratinguserId, aes(xintercept=mean(avg_rating)), col="red")+geom_vline(data=avgmovieratinguserId, aes(xintercept=median(avg_rating)), col="blue")+ggtitle("distribution of average rating by userId (n()>=100);blue line= median, red line= average")+theme(plot.title=element_text(size=10))

distribavgratingmovieId

distribavgratinguserId

```

The definition of a user and a movie title (and associated movieId) are straightforward and do not need much further description to add to understanding these data points.  However, the genre descriptors and the timestamps were investigated to determine what information they contribute to understanding the dataset.

**The genres**

The genres are single or multiple word descriptors of the movie content.  In all, there are 20 different genre descriptors, used as single word descriptors or combined in groups of up to 8 different descriptors.   For example, Dead Poets Society (1989) is described as a Drama.  However, many movies are described by more than one genre:  Lion King, The (1994) is described by five genres (Adventure|Animation|Children|Drama|Musical).  As described above, there are 797 unique genre word combinations (which includes seven movies with no genre description and one movie (with 256 ratings) with eight genres describing its content (Host, The (Gwoemul)(2006)).  The top five genre descriptors are shown in the following table (note that this table does not tell us that the largest categories are rated highly for all movies included in the category;  it merely tells us how many movies are described by the genre)(genre_count in this table represents the number of words in the genre description):

```{r, genre_counts, echo=FALSE, eval=TRUE, fig.cap="Top five genres by rating count"}
genre_descriptors <- edx[,c(6)] %>% distinct(genres) %>% mutate(count_descriptor=(str_count(genres,pattern="\\|")+1))

##find the most popular genres by rating count
favourite_genres <- edx[,c(3,6)] %>% group_by(genres) %>% summarise(count_ratings=n()) %>% left_join(genre_descriptors, by="genres") %>% arrange(desc(count_ratings)) %>%head(5)

kable(favourite_genres, caption="Top 5 genres by rating count")

```

To try to understand the distribution of the genres in general terms, a histogram was created that plotted the genres (by count) for each userId (for all userId with greater than 100 ratings).  As there are 797 different genres, it was necessary to create an number index in order to create a histogram in readable form.  The index number was assigned to the genre list in alphabetical order…so that genres beginning with Action were found in the lower numbers of the index and the genres beginning with War were at the end of the index.

The histogram reveals that some genres are almost never reviewed (the white space on the histogram has empty data; the dark blue indicates a low count of ratings).  As indicated in the tables, there are only a few categories that stand out for preferred viewing.  But this histogram does not indicate that these genres would necessarily be correlated to a good rating. 

```{r, genres-histogram-1, echo=FALSE, eval=TRUE, fig.dim=c(7,5), warning=FALSE, message=FALSE}
##find out if genre affects the user or ratings
## need userid, rating and an index which identifies the genres (and group the genres by the first word descriptor)
##first up, devise a useful index number for the genres
##start with genre_descriptors 

partoneindex <- genre_descriptors %>% mutate(first=word(genres, sep="\\|")) %>% arrange(first, count_descriptor)

uniquefirst <- partoneindex %>% distinct(first) %>% mutate(id=row_number())

##now create the index
index <- partoneindex %>% mutate(genre_index=row_number()) %>% left_join(uniquefirst, by="first") 

countid <- index %>% group_by(id)%>%summarise(n()) %>% arrange(id)
##index2 <- index %>% unite(col="idf", c("id","id2"), sep="")
indexfinal <- index[,c(1,4)]

##create a 2d histogram of rating by userId for each genre

genredata<- edx[,c(1,3,6)] %>% left_join(indexfinal, by="genres") %>% group_by(userId) %>% filter(n()>100) 

genredata2 <- genredata[,c(1,2,4)] %>% group_by(userId,genre_index) %>% summarise(count=n()) 

ggplot(genredata2, aes(x=genre_index,y=userId))+geom_bin2d(bins=120)+scale_fill_continuous(type="viridis")+theme_bw()+ggtitle("Genre (by rating count) by userId")

```

Attempting to visualize the histogram a ‘good’ rating, the same plot was created, but this time, only records with a rating equal to five were included in the visualization.  This time, with the somewhat smaller sample size, it remains clear that some genres are preferred by all users and rated more highly and that these genres are also the genres with more ratings (and therefore, selected for viewing).

```{r, genres-histogram-2, echo=FALSE, eval=TRUE, fig.dim=c(7,5), warning=FALSE, message=FALSE}

#now, see if the ratings which are above the edx mean have an impact on that heat map
##select the count for all rating above the edx mean

meanrating <- mean(edx$rating)
genredata3 <- genredata[,c(1,2,4)] %>% filter(rating==5) %>% group_by(userId,genre_index) %>% summarise(count=n())

ggplot(genredata3, aes(x=genre_index,y=userId))+geom_bin2d(bins=120)+scale_fill_continuous(type="viridis")+theme_bw()+ggtitle("Genre (by rating count) by userId; rating=5")

```

That  higher counts of ratings seem to coincide with more favourable ratings is easy enough to explain:  if the reviewers favour a  genres they are more likely to select those genres for viewing so that review counts alone may reflect a bias in tastes.

The genre descriptors are in alphabetical order.  That means that a movie described by Comedy|Drama|Romance is not mostly a Comedy any more than it is a Comedy with some Romance or primarily a Romance with Drama and a touch of Comedy.  Some wordy genre descriptors are rarely observed:  Action|Adventure|Animation|Comedy|Sci-Fi is applied to only one **not** well known movie (Dead Leaves (2004)) which has only three reviews (the reviews weren’t bad…two 4.5 and one 3.0).  

The matter of genres required a bit more research. So, I went to Netflix!  Netflix has a number of movie genres so viewers can select a movie based on their genre preferences.  A movie that falls under multiple genre descriptors will be found on a number of different genre menu selections.  For example,  the movie Sahara (2005) is found listed as a Comedy under the Action menu selection.  It is also found as an Action&Adventure based on Books movie under the Comedies menu selection.  (Good work Netflix movie recommendation programmers!).  It seems that in a movie recommendation model, the genre descriptor would be more suited to nonlinear decision tree programming than to a linear least squares model.  However, genre preference will be related to userId and may be a good rough indicator for movie recommendations even in a linear model.


**The Timestamp**

As described above, there are a little over 6.5 million time stamps in the database.  The raw data is presenting a timestamp which represents the time and date on which the rating was provided. The units are seconds since January 1, 1970.  Investigation of this variable is easier if the timestamp were presented in a form more easily interpreted.  Therefore, the timestamps were converted to year, month, week , day and hour of day, to see if there are patterns to be discovered which explain ratings.

```{r, time-code, echo=FALSE, eval=TRUE, message=FALSE}
small <- edx[,c(1,2,3,4,6)] %>% left_join(indexfinal, by="genres")
smaller <- small[,c(1,2,3,4,6)]
movietime <- smaller %>% mutate(time= as_datetime(timestamp), year=year(time), month=month(time), mday=mday(time), wday=wday(time), hour=hour(time)) 

## look for correlations with ratings (count, average rating) by year, month, mday, wday,hour
#install.packages("PerformanceAnalytics")

yeardate <- movietime[,c(3,7)] %>% group_by(year) %>% summarise(count=n(), avgrating=mean(rating))
uniqueyear <- yeardate %>% distinct(year)

```


Viewing the data of count of ratings by year reveals that the values in 1995 and 2009 are obvious outliers;  removing these years from the dataset shows more linear relationship between year and count of movie ratings but it is still pretty rocky. 

```{r, year-lollipop, eval=TRUE, echo=FALSE, fig.dim=c(7,4), message=FALSE}
yearplotcount <- ggplot(yeardate, aes(x=as.factor(year), y=count, label=round(count, digits=0)))+geom_point()+geom_segment(aes(x=as.factor(year), xend=as.factor(year), y=0, yend=count))+geom_text(size=4, vjust=-0.75)+xlab("Year")+ylab("Count of rating (in thousands)")+ggtitle("Count of rating by Year of rating")
yearplotavgrate <- ggplot(yeardate, aes(x=as.factor(year), y=avgrating, label=round(avgrating, digits=3)))+geom_point()+geom_segment(aes(x=as.factor(year), xend=as.factor(year), y=0, yend=avgrating))+geom_text(size=4, vjust=-0.75)+xlab("Year")+ylab("Average rating")+ggtitle("Average rating by Year of rating")

yearplotcount
yearplotavgrate
```

The plot of average rating vs year (above)  indicates that there is a slight decline in average rating over time, by year.  

Next, we have a plot of average rating vs count of ratings.  

```{r, count-histogram, echo=FALSE, eval=TRUE, fig.dim=c(7,4), message=FALSE}
yeardate2 <- movietime[,c(3,7)]

ggplot(yeardate2, aes(x=year, y=rating))+geom_bin2d()+scale_fill_viridis_c(option="magma")+theme_bw()+ggtitle("Count of rating score by year") 
```

We can see that over the years, there are fewer counts of ratings four and over.  The view is obscured by the fact that the rating system changed in 2003, so that half grades (e.g. 0.5, 1.5, 2.5, 3.5, 4.5) are possible.  Such a change might impact the user’s ratings if a rating option between 3.0, 3.5 or 4.0 were easier (3 might bump up to 3.5; 4 might bump down to 3.5 and so on), and therefore the user appears to become more critical over time.  We can see that the users are not handing out many scores of ‘5’ or ‘4’ after the rating system changes.   Also, we know that counts in 1995 and 2009 are outliers; therefore, these years are not good indicators of a trend in category of rating counts. 

Release year of the movie has some interesting information to share.  First, looking at count of rating by release year, it seems that the users like to review some of the really old movies.   It also seems that there is less and less interest in reviewing movies released after 1995. 

```{r, release-year, echo=FALSE, eval=TRUE, fig.dim=c(7,4)}
##first, you need to extract the year of release

releaseyear <- edx %>% mutate(release_year = as.numeric(str_sub(title,-5,-2))) %>% left_join(indexfinal, by="genres")
releaseyeargrouped <- releaseyear[,c(3,7)] %>% group_by(release_year) %>% summarise(avg_rating=mean(rating), count=n())
justreleaseyear <- releaseyear[,c(5)] %>% distinct(title) %>% mutate(release_year = as.numeric(str_sub(title,-5,-2))) %>% group_by(release_year) %>% summarise(count=n()) 
ryearrelease <-ggplot(justreleaseyear, aes(x=as.factor(release_year), y=count))+geom_point()+geom_segment(aes(x=as.factor(release_year), xend=as.factor(release_year), y=0, yend=count))+xlab("Release Year")+ylab("Count of titles")+ggtitle("Count of titles by Release Year")+scale_x_discrete(breaks=c(1915, 1925, 1935, 1945, 1955, 1965, 1975, 1985, 1995, 2005, 2015))##+scale_y_continuous(labels="comma")
yearplotcountr <- ggplot(releaseyeargrouped, aes(x=as.factor(release_year), y=count))+geom_point()+geom_segment(aes(x=as.factor(release_year), xend=as.factor(release_year), y=0, yend=count))+xlab("Release Year")+ylab("Count of rating")+ggtitle("Count of rating by Release Year of rating")+scale_x_discrete(breaks=c(1915, 1925, 1935, 1945, 1955, 1965, 1975, 1985, 1995, 2005, 2015))##+scale_y_continuous(labels="comma")
yearplotavgrater <- ggplot(releaseyeargrouped, aes(x=as.factor(release_year), y=avg_rating, label=round(avg_rating, digits=3)))+geom_point()+geom_segment(aes(x=as.factor(release_year), xend=as.factor(release_year), y=0, yend=avg_rating))+xlab("Release Year")+ylab("Average rating")+ggtitle("Average rating by Release Year of rating")+scale_x_discrete(breaks=c(1915, 1925, 1935, 1945, 1955, 1965, 1975, 1985, 1995, 2005, 2015))

ryearrelease
yearplotcountr

```
 

It looks like there has been no decline in the number of movies release every year since 1995 (very roughly speaking).  We could suggest that users are just not increasing the number of reviews in proportion to the number of movies released. However, if it is true that users are less and less interested in reviewing more movies as more movies are released , then are recommendations based upon movie reviews going to be biased towards the tastes and preferences of those people who like to review movies and who take the time to fill in movie rating forms? Difficult to answer that question, as we do not have any information about the personal characteristics of the reviewers.

Looking for a trend in the average rating by release year, a brief view of the visual presentation reveals a confusing story: The average rating has shown a noticeable decline, starting in about 1985.  Which is a bit confusing because the trend presented here has quite a few decades to establish.  So, I am wondering if the change in the rating system (as described earlier) has resulted in a more critical review.

```{r, echo=FALSE, eval=TRUE, fig.dim=c(7,4)}
yearplotavgrater
```

A bit of research reveals that ‘blockbuster’ movies are released on a fairly predictable timeline during the year^[https://daveswallet.com/what-are-the-best-months-to-release-box-office-movies-and-why/].   Movies are typically released in June, July and August to take advantage of the fact that children are out of school and parents are looking for summer vacation activities.  The next sizeable release is November and December for movies looking to win at the Oscar awards and to take advantage of the Christmas holiday season.  With this in mind, the analysis of the timestamp, by month of rating, was carried out to see if there is a seasonal effect to the count of ratings or average rating.

```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, fig.dim=c(7,4)}
months <- movietime[,c(3,8)] %>% group_by(month) %>% summarise(avg_rating=mean(rating), count=n())
monthplotsc <- ggplot(months, aes(x=as.factor(month), y=count))+geom_point()+geom_segment(aes(x=as.factor(month), xend=as.factor(month), y=0, yend=count))+xlab("Month")+ylab("Count of rating")+ggtitle("Count of rating by Month")
monthplotsr <- ggplot(months, aes(x=as.factor(month), y=avg_rating))+geom_point()+geom_segment(aes(x=as.factor(month), xend=as.factor(month), y=0, yend=avg_rating))+xlab("Month")+ylab("Average rating")+ggtitle("Average rating by Month")

monthplotsc

```

The visual above indicates, very roughly, that there may in fact be a seasonal component to the count of movie ratings.  There is a gentle uptick in the count in the month of July (blockbuster releases) and then another uptick in November and December (Oscar worthy movies and then Christmas).  It makes a bit of sense to discover that the movies out for release, looking for an Oscar would attract the users who like to rate movies, so the count of ratings goes up to its highest point.  We don’t have data on the users who rate the movies, so we can only guess at the characteristics of movie reviewers and how their preferences might be influencing the dataset.

A density histogram was created to see if a pattern of genres selection could be observed.  Per the information presented in the genres section, we do have preferences.  Would these preferences for a preferred genre be different in the summer months versus the months of fall (the ‘blockbuster’ release months versus the ‘Oscar contenders’ months). It is difficult to find a visualization technique that would take the entire data set and summarize it into something easy to read.  Therefore, the histogram was slimmed down a bit by selecting for genres with an average rating greater than 3.0.

```{r, echo=FALSE, eval=TRUE, fig.dim=c(7,4), warning=FALSE, message=FALSE}
genresbymonth <- movietime[,c(3,5,8)]  %>% group_by(month, genre_index) %>% summarise(count=n(), avgrating=mean(rating)) %>% filter(avgrating>=3.0) 
histgenresbymonth <- ggplot(genresbymonth, aes(x=month, y=genre_index))+geom_bin2d()+scale_fill_viridis_c(option="inferno", direction=1)+theme_bw()+ggtitle("count of genres by month, average rating >= 3.0") +scale_x_continuous(breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))+ylab("genre by index - alphabetical order")

histgenresbymonth

```

Looking at the histogram, we see that the genre preferences remain fairly steady over the course of a year.  There is a bit of movement in the last quarter for the genres in the 200 range (looking at my index, these are the genres described by Action movies of various description…Action movies are described by index numbers from 2 to 253).  Our favourite genres always seem to be included in the Drama categories:  the only bright yellow line across all months is in the Drama genres(index number 658-731). 

```{r, echo=FALSE, eval=TRUE, fig.dim=c(7,4), warning=FALSE, message=FALSE}
monthplothist <- ggplot(months, aes(x=as.factor(month), y=avg_rating))+geom_bin2d(show.legend=FALSE)+scale_fill_viridis_c(option="inferno", direction=1)+theme_bw()+ggtitle("average rating by month") +scale_x_discrete(breaks=c(1,2,3,4,5,6,7,8,9,10,11,12))+ylab("average rating by month")

monthplothist

```

The average rating by month, when displayed on a scale that begins at 3.475, shows a discernible pattern.  If we are expecting that the Oscar contenders are released in October and November, it could be that the ratings reflect at the least, the expectation of better quality and perhaps a higher rating.  

I examined the day of the month, day of the week and hour of the day, looking for useful patterns in count or average rating and did not find anything that I believed would be useful for my linear model.  The work is not presented in this report.

Summing up what has been learned from the timestamp and the year of release of the movie:
Reviewing the timestamp variable by disaggregating it into year and month is interesting and yields interesting bits of information about the database.  There is very little revealed which indicates that a time variable based upon the day or hour of the review would improve a movie recommendation system. There is some indication that month of the year as related to blockbuster releases and Oscar contending releases, may impact upon average rating and counts of rating.  The time variables indicate when movie ratings are counted and higher counts seem to indicate higher rating scores, as evidenced by the observation that the Oscar contender months, October and November, have an increase in both count of ratings and average rating.  There is an observed change in rating practices after 2003 which also seems to have an impact on scoring patterns.  There is some indication, based upon the year of release and year of rating, which indicate that there are shifting patterns in tastes, as exhibited by a general trend to lower average rating.   

```{r, remove-files, include=FALSE}
rm(avgmovieratingmovieId, avgmovieratinguserId, barcountbyhour, countid, dayshours,distribavgratingmovieId,
   distribavgratinguserId, favourite_genres, genre_descriptors, genredata, genredata2, genredata3, genresbymonth,
   highlyratedgenres, histbymonday, histgenresbymonth, index, justreleaseyear, mdayplotsc, mdays, measures,
   monthplothist, monthplotsc, monthplotsr, movietime, partoneindex, releaseyear, releaseyeargrouped, ryearrelease,
   shapiro_avgrate, shapiro_count, small, smaller, unique_count, uniquefirst, uniqueyear, yeardate, yeardate2,
   yearplotavgrater, yearplotcount, yearplotcountr)
```


**The Regression Models**

The regression models were set up as described in https://rafalab.github.io/dsbook/large-datasets.html#movielens-data and the BellKor Solution to the Netflix Grand Prize. My procedure followed that described in the course text .  I followed the procedures in the course text to familiarize myself with the R programming required and to document the improvements to the model possible when straightforward improvements to the model were put through the R programming.  

The text describes the estimation of an RMSE obtained by estimating the mean of all test set ratings (mu_hat as the predicted_rating) and then estimating the RMSE as:  sqrt(mean((true_ratings - predicted_ratings)^2)), where the true_ratings are the ratings of each movie i by each user u.  The RMSE is interpreted similar to a standard deviation.  If the standard error is greater than 1, the regression model generating the value is not good.

The BellKor Solution describes attempts at estimating the time effects of the changing tastes for the movies and the changing tastes of the individual movie raters.  To account for changing tastes in movies over time, the BellKor team binned the time series into ten week intervals to achieve a beta for movieId that would have a preference for the movie plus a time factor that takes into account preferences over time.  Their model was described by:

bi(t) = bi +bi_Bin(t)

In my model, I have binned the time of review into ten week intervals to get a beta estimate of ‘bin’, b_b.  As noted previously in the data analysis section, there appears to be a shift in the rating scheme in 2003.  Also shown in the data analysis section was a decline in rating score over time, as illustrated by the plots of the average rating by year of release of the movie. Also observed was that there are shifts in preferences by season or 'months', so perhaps binning in 10 week segments would pick up the seasonal influences.  So, by binning the data, it may be possible to improve the RMSE as it captures a change in taste of movies and possibly takes into account the sudden shift in the rating that took place as the scoring scheme was adjusted.

The BellKor Solution also describes a time element of the model for individual movie raters, described by b_u(t) = bu +αu · dev_u(t),  where  dev_u(t) = sign(t −t_u)·|t −t_u| β , where is a constant (as estimated by BellKor) of β = 0.4 and t is ‘day of rating’ and t_u is the average day of rating for user u.  I estimated a variable dev_u(t), but found that my computer does not have the RAM nor sufficient processor speed to successfully process this variable.  So, it was not used in my model.  Although, it does make sense as a variable; I wish I could have seen it incorporated sucessfully.

The BellKor team also estimated a variable F_u,i the overall number of ratings that user u gave on day t_u,i.  The analysis of the data, as previously described, supports the notion that users tend to rate a large count of movies one a single day.   The BellKor team has explained that the practice of reviewing many movies at one go may influence ratings:  movie watchers rating movies remember movies they like in very positive terms and movies they dislike in very negative terms.  Therefore, the frequency variable may pick up biases that exaggerate a strong like or dislike of a movie and could therefore explain user bias more thoroughly.  Therefore I calculated my own ‘freq’ variable and its beta value b_F, where ‘freq’ is described by 

f_u,i = (log_a (F_u,i)).

In the edx movielens database, I found that the mean of ratings per submission of ratings by users was a little above 21 ratings per submission (in other words, the ratings were lumped together on one day an average of 21 ratings submitted at one day).  Estimating the effect of lumping ratings together  was straightforward enough and could explain some variability in the ratings by a beta described by b_F for variable ‘freq’.

Most importantly, before splitting the edx movielens database into the training set and the test set, the independent numeric variables were scaled to a z-distribution, as recommended by a number of programming guides^[https://www.programmingr.com/examples/neat-tricks/using-the-scale-function/] ^[https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/].  Scaling the data to a z-distribution turned out to be the best way to obtain good beta estimates.  The following variables were scaled using the formula (x-mean(x)/standard deviation (x)):  rating, freq.

Finally, the data set was split into two segments (not including the adjustments made to create the ‘validation’ data set, as described in the course material ); the train set and the test set.   The test set consists of 20% of the edx movielens dataset.   

**The Results**

```{r, regression_data, include=FALSE, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
timeset <- edx %>% mutate(time= round_date(as_datetime(timestamp),"day")) 

##find the minimum date in order to create the time bins

min<- min(timeset$time)

timestamplist <- timeset[, 7] %>% distinct() %>% arrange(time) %>% mutate(bin=1)

### taking note of the Netflix Prize paper, bins were arranged to be approximately 10 weeks
bin_days <- 70

## make a database of bins and then fit it to the timestamplist
##step by step - because this is a new process
smalltimelist <- timestamplist %>% mutate(newt=as.numeric(time))
newt <- as.matrix(smalltimelist[,3], nrow=4633, ncol=1)
class(newt)

df <- matrix(1:75, nrow=75, ncol=1)

newdf <- as.data.frame(df) %>% mutate(bin=min +days(70*(V1-1)), binend=bin+days(70))

newdf1 <- newdf %>% mutate(binendd = as.numeric(binend))
newdf2 <- as.vector(newdf1[,4])

newdf3 <- newdf %>% mutate(binb=as.numeric(bin))
newdf4 <- as.vector(newdf3[,4])
class(newdf2)

length(newdf2)
##merge timestamplist on the binend of newdf...

categories <- data.frame(newt, bin=cut(newt, breaks=c(newdf4),labels=FALSE, include.lowest=TRUE, right=TRUE,dig.lab=8))
categories2 <- categories %>% mutate(thing=as.POSIXct(newt, origin="1970-01-01"))

##now, join the two tables categories2 and smalltimelist by newt
smalltimelist2 <- smalltimelist[,c(1,3)]
allnewt <- smalltimelist2 %>% left_join(categories2, by="newt") %>% mutate(timed =as.POSIXct(time, tryFormats=c("%Y-%m-%d")))

##now, try to join allnewt back into the timeset database by time

timesetbinned <- timeset %>% left_join(allnewt, by="time")
str(timesetbinned)
edxbinned <- timesetbinned[,c(1,2,3,4,6,7,9, 10, 11)]


##as an aside....count of rating by userId on a particular days is interesting...many movies, one day..too often the case
testthis <- edxbinned %>% filter(userId==13) %>% group_by(userId, time) %>% summarise(countid = n())
mean_count_per_day <- edxbinned %>% group_by(userId, time) %>% summarise(countd=n())
mean(mean_count_per_day$countd)
##create the frequency variable...the log of the number of ratings on a single day by one user

frequencyrate <- edxbinned %>% group_by(userId, time) %>% summarise(countd=n()) %>% mutate(logF=log(countd))

##scale the frequency variable to a z distribution
sdlogF <- sd(frequencyrate$logF)
meanlogF <- mean(frequencyrate$logF)
frequencyratescaled <- frequencyrate %>% mutate(scale_logF=(logF-meanlogF)/sdlogF)
minF<-min(frequencyrate$logF)
minF

edxbinned4 <- edxbinned %>%  left_join(frequencyratescaled[,c(1,2,5)], by=c("userId", "time")) %>% mutate(ratings=rating)

###scale the ratings variable to a z distribution
sdrating <- sd(edxbinned4$ratings)
meanrating <- mean(edxbinned4$ratings)

edxbinned3 <- edxbinned4 %>% mutate(rating=(ratings-meanrating)/sdrating)
##test for mean at zero
mrating<- mean(edxbinned3$rating)


##enough variables....create the train set and the test set
##reduce the size of the table
edxshort <- edxbinned3[,c(1,2,3,5,7,10)]


set.seed(755)
test_index <- createDataPartition(y=edxshort$rating, times=1, p=0.2, list=FALSE)

train_set <- edxshort[-test_index,]
test_set <- edxshort[test_index,]

test_set<- test_set %>% semi_join(train_set, by = "movieId") %>% semi_join(train_set, by = "userId")

###remove the large files that are no longer required
rm(edx, edxbinned, edxbinned3, edxbinned4, edxshort, edxsmall, frequencyrate, frequencyratescaled, mean_count_per_day, timeset, timesetbinned)


```



The first regression is a measure of the model when we predict the same rating for all movies, regardless of the user or the tastes for the movie.  This model, as in the course material, is the 'naive model'.

```{r, begin-model, include=FALSE, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
RMSE <- function(true_ratings, predicted_ratings){sqrt(mean((true_ratings - predicted_ratings)^2))}

mu_hat <- mean(train_set$rating)
mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse

##create a results table
rmse_results <- tibble(method="naive model", RMSE=naive_rmse)
format(rmse_results, digits=5, nsmall=5)
```


The second regression model obtains an RMSE resulting when the effects of individual movie ratings are included.  The distribution of the movie effect beta appears to be normally distributed, although slightly skewed.

```{r, movie-effect, echo=FALSE, eval=TRUE, fig.dim=c(7,4), message=FALSE, warning=FALSE}
###create the b_i for the movieId effect 
mu <- mean(train_set$rating)

movie_avgs <- train_set %>% group_by(movieId) %>% summarise(b_i = mean(rating-mu))

###create the b_i for the movieId effect 
mu <- mean(train_set$rating)

movie_avgs <- train_set %>% group_by(movieId) %>% summarise(b_i = mean(rating-mu))

qplot(b_i, data=movie_avgs, bins=35, color=I("black"))+ ggtitle("b_i estimates for movie effect")+xlab("estimates of b_i")+ylab("count of b_i estimate")

predicted_ratings <- mu + test_set %>% left_join(movie_avgs, by=c("movieId")) %>% pull(b_i)

movie_effect_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect", RMSE=movie_effect_rmse)



```

The RMSE for a model including the movie effect b_i is reduced from the initial ‘naïve’ model (as seen in the final results table at the end of this section).

The next model estimated a beta for the binning effect, the change in the tastes for movies of different sorts, as measured over time.  As can be seen, the distribution of the bin effect does not appear to be normal;  there appears to be a binormal distribution, with one evident outlier. As discussed earlier, there was a change in the scoring of movies in 2003…it could be that the binning effect has captured the change, resulting in a shift in the beta.  If this is the case, then the binning effect is misidentified and has not properly captured shifts in rating (or movie preferences) over time.  If there were more time available to me, I would consider adding a dummy variable into the model which might capture the shift in scoring in 2003, which might lead to a better measure of the binning effect beta.  

```{r, bin-effect, echo=FALSE, eval=TRUE, fig.dim=c(7,4)}

#include bin effect and movie effects
bin_avgs <- train_set %>% left_join(movie_avgs, by ="movieId") %>% group_by(bin) %>% summarise(b_b = mean(rating-mu-b_i))
##put in y_hat (u,i) = u_hat + b_hat(i)

predicted_ratings <- test_set %>% left_join(movie_avgs, by="movieId")%>%left_join(bin_avgs, by=c("bin")) %>% mutate(pred=mu+b_i+b_b)%>%pull(pred)

movie_and_bin_effect_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include bin and movie effect", RMSE=movie_and_bin_effect_rmse)

qplot(b_b, data=bin_avgs, bins=35, color=I("black"))+ ggtitle("b_b estimates for movie effect")+xlab("estimates of b_b")+ylab("count of b_b estimate")


```

The third regression includes the effects of an individual user on the movie ratings.  This model takes into account the fact that different movie watchers have different preferences or rating schemes and therefore don’t rate each movie on exactly the same criteria.  The distribution for the user effect is illustrated in a histogram; the distribution appears to be normal.  Noted is that the user effect has the single greatest impact of any explanatory variable on reducing the RMSE in the model.

```{r, user-effect, eval=TRUE, echo=FALSE, fig.dim=c(7, 4)}

user_avgs <- train_set %>% left_join(movie_avgs, by="movieId") %>% left_join(bin_avgs, by="bin") %>% group_by(userId) %>% summarise(b_u = mean(rating- mu - b_i - b_b))

predicted_ratings <- test_set %>% left_join(movie_avgs, by="movieId") %>% left_join(bin_avgs, by="bin")%>%left_join(user_avgs, by='userId') %>%mutate(pred=mu+b_i+b_b+b_u)%>%pull(pred)
user_effect_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect + bin effect + user effect", RMSE=user_effect_rmse)

qplot(b_u, data=user_avgs, bins=35, color=I("black"))+ ggtitle("b_u estimates for user effect")+xlab("estimates of b_u")+ylab("count of b_u estimate")

```

The next model includes the frequency of ratings effect. The frequency of ratings effect  beta is shown in the histogram below: 

```{r, freq-effect, echo=FALSE, eval=TRUE, fig.dim=c(7,4)}
freq_avgs <- train_set%>% left_join(movie_avgs, by="movieId") %>% left_join(bin_avgs, by="bin") %>% left_join(user_avgs, by="userId")%>% group_by(scale_logF) %>% summarise(b_F = mean(rating- mu - b_i - b_b - b_u))

predicted_ratings <- test_set %>% left_join(movie_avgs, by="movieId") %>% left_join(bin_avgs, by="bin")%>%left_join(user_avgs, by='userId') %>% left_join(freq_avgs, by="scale_logF") %>%mutate(pred=mu+b_i+b_b+b_u+b_F)%>%pull(pred)
freq_effect_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect + bin effect + user effect+freq effect", RMSE=freq_effect_rmse)

qplot(b_F, data=freq_avgs, bins=35, color=I("black"))+ ggtitle("b_F estimates for frequency of rating effect")+xlab("estimates of b_F")+ylab("count of b_F estimate")

```

The distribution of the freq beta appears to be normal, with very little spread.  The frequency variable contributes to the reduction of the RMSE, but certainly does contribute to a dramatic drop in the RMSE as was documented by the BellKor Solution.

The regression model finally includes the genre effect.  A histogram of the distribution of the genre effect beta is below.  The distribution appears to be normal.   It therefore makes sense to see that when the user effect is present, the genres effect shows a positive effect;  different users have different genres preferences and this may show up even in the genres of movies selected for reviewing.  

```{r, genres-effect, eval=TRUE, echo=FALSE, fig.dim=c(7,4)}
genre_avgs <- train_set %>% left_join(movie_avgs, by="movieId") %>% left_join(bin_avgs, by="bin") %>% left_join(user_avgs, by="userId") %>% left_join(freq_avgs, by="scale_logF") %>% group_by(genres) %>% summarise(b_g = mean(rating- mu - b_i - b_u-b_b-b_F))

predicted_ratings <- test_set %>% left_join(movie_avgs, by="movieId") %>% left_join(user_avgs, by='userId') %>% left_join(bin_avgs, by="bin")%>% left_join(freq_avgs, by="scale_logF")%>% left_join(genre_avgs, by="genres") %>% mutate(pred=mu+b_i+b_u + b_b+b_F+ b_g)%>%pull(pred)
genres_effect_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect + bin effect + user effect+freq effect+genre effect", RMSE=genres_effect_rmse)

qplot(b_g, data=genre_avgs, bins=35, color=I("black"))+ ggtitle("b_g estimates for genres effect")+xlab("estimates of b_g")+ylab("count of b_g estimate")

```

As described in the course text, a regularized model takes into account that a movie with few reviews but a very high rating will have a misleading and over- sized impact on the sample average variable estimates.   The bi estimates for such obscure movies will have a larger variance (they are ‘noisy’) leading to an increase in the RMSE of the model.  To remove the influence of these records, they are treated with a penalty (as represented by lambda) which reduces the value of a small rating count b(i) towards zero, such that it has little impact on the average sample beta estimate and reduces the error term of the model, improving the RMSE.  The equation to obtain the regularized b(i) is:

b_hat_i(λ)=(1/  λ+i)*∑(Y_u,i - u_hat), where the summation goes from u=1 to number of ratings for movie i. 
 
To obtain an estimate of lambda for the regularized equation, a program is run to find the lambda which minimizes the RMSE.  

The grid below finds the lambda for the two largest contributors to the model:  b_i, the beta for the movie effect and  b_u, the beta for users.  As can be seen, a lambda of approximately 5.0 will result in the best RMSE. 

```{r, lambda-fig, echo=FALSE, eval=TRUE, fig.dim=c(7,5)}

lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)+ggtitle("find lambda for movie effect + user effect") 

```


In the BellKor Solution paper, a separate lambda was estimated for each variable.  I did not attempt to find a separate lambda for each variable, as the lambda of 5 estimated for the combined movie effect and the user effect appeared to be quite suitable. 

The RMSE for all equations is shown below.  From what we see so far, the model including movie effect+bin effect + user effect+frequency of rating + genres effect shows the best RMSE; standardizing the variables with a lambda equal to 5 brings the RMSE to the lowest value of 0.816268.

```{r, standardized, include=FALSE, echo=FALSE, eval=TRUE}

##take the value of lambda from the plot and install into the lambda value for standardizing

lambda <- 5.0

## get movie_reg_avgs and user_reg_avgs
mu <- mean(train_set$rating)
movie_reg_avgs5 <- train_set %>% group_by(movieId) %>% summarise(b_i = sum(rating - mu)/(n()+lambda), n_i=n())

#tibble(original= movie_avgs$b_i, regularized=movie_reg_avgs$b_i, n=movie_reg_avgs$n_i) %>% ggplot(aes(original, regularized, size=sqrt(n)))+geom_point(shape=1, alpha=0.5)+ggtitle("Regularized movie effect")

bin_reg_avgs5 <- train_set %>% left_join(movie_reg_avgs5, by="movieId") %>% group_by(bin) %>% summarise(b_b = sum(rating - mu - b_i)/(n()+lambda), n_u=n())

user_reg_avgs5 <- train_set %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(bin_reg_avgs5, by="bin") %>% group_by(userId) %>% summarise(b_u = sum(rating - mu - b_i - b_b)/(n()+lambda), n_u=n())

#tibble(original= user_avgs$b_u, regularized=user_reg_avgs$b_u, n=user_reg_avgs$n_u) %>% ggplot(aes(original, regularized, size=sqrt(n)))+geom_point(shape=1, alpha=0.5)+ggtitle("Regularized user effect")

freq_reg_avgs5 <- train_set %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(user_reg_avgs5, by="userId") %>%left_join(bin_reg_avgs5, by="bin") %>% group_by(scale_logF) %>% summarise(b_F = sum(rating - mu - b_i -b_b -b_u)/(n()+lambda), n_g=n())

genre_reg_avgs5 <- train_set %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(user_reg_avgs5, by="userId") %>%left_join(bin_reg_avgs5, by="bin") %>% left_join(freq_reg_avgs5, by="scale_logF") %>% group_by(genres) %>% summarise(b_g = sum(rating - mu - b_i -b_b -b_u-b_F)/(n()+lambda), n_g=n())
#tibble(original= genre_avgs$b_g, regularized=genre_reg_avgs$b_g, n=genre_reg_avgs$n_g) %>% ggplot(aes(original, regularized, size=sqrt(n)))+geom_point(shape=1, alpha=0.5)+ggtitle("Regularized genres effect")



```


```{r, more, include=FALSE, echo=FALSE, eval=TRUE}
##find the new RMSE
#just movieId
#qplot(b_i, data=movie_reg_avgs, bins=15, color=I("black"))+ ggtitle("b_i estimates for regularized movie effect")+xlab("estimates of regularized b_i")+ylab("count of regularized b_i estimate")
predicted_ratings <- mu + test_set %>% left_join(movie_reg_avgs5, by='movieId') %>% pull(b_i)
reg_movie_effect_lambda5 <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect, regularize with lambda = 5", RMSE=reg_movie_effect_lambda5)


##movie id and bin effect

predicted_ratings <- test_set %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(bin_reg_avgs5, by='bin') %>%mutate(pred=mu+b_i+b_b)%>%pull(pred)
reg_movie_and_bin_lambda5 <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect + bin effect, regularize with lambda = 5", RMSE=reg_movie_and_bin_lambda5)


##movid id and user effect and bin effect

predicted_ratings <- test_set %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(user_reg_avgs5, by='userId') %>% left_join(bin_reg_avgs5, by="bin") %>% mutate(pred=mu+b_i+b_u + b_b)%>%pull(pred)
reg_movie__user_genres_lambda5 <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect + bin effect + user effect, regularize with lambda = 5", RMSE=reg_movie__user_genres_lambda5)

##movie id and user effect and frequency effect and bin effect
predicted_ratings <- test_set %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(user_reg_avgs5, by='userId') %>% left_join(bin_reg_avgs5, by="bin") %>% left_join(freq_reg_avgs5, by="scale_logF") %>% mutate(pred=mu+b_i+b_u + b_b +b_F)%>%pull(pred)

reg_movie__user_bin_freq_lambda5 <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect + user effect + bin effect+freq effect, regularize with lambda = 5", RMSE=reg_movie__user_bin_freq_lambda5)

##movie id, user effect, bin effect, frequency effect, genres effect

predicted_ratings <- test_set %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(user_reg_avgs5, by='userId') %>% left_join(bin_reg_avgs5, by="bin") %>% left_join(freq_reg_avgs5, by="scale_logF") %>% left_join (genre_reg_avgs5, by="genres") %>% mutate(pred=mu+b_i+b_u + b_b +b_F+b_g)%>%pull(pred)

reg_movie__user_bin_freq__genres_lambda5 <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- add_row(rmse_results, method="include movie effect + user effect + bin effect+freq effect+genres effect, regularize with lambda = 5", RMSE=reg_movie__user_bin_freq__genres_lambda5)


```


```{r, validation, include=FALSE, echo=FALSE, eval=TRUE}


####
#####test the validation set

####first step is to create the bins for the validation set
validationtimeset <- validation %>% mutate(time= round_date(as_datetime(timestamp),"day")) 
validationbinned <- validationtimeset %>% left_join(allnewt, by="time")
validationbin <- validationbinned[,c(1,2,3,4,6,7,9, 10, 11)]
##create frequency variable
validfrequencyrate <- validationbin %>% group_by(userId, time) %>% summarise(countd=n()) %>% mutate(logF=log(countd))

##scale the frequency variable to a z distribution
validsdlogF <- sd(validfrequencyrate$logF)
validmeanlogF <- mean(validfrequencyrate$logF)
validfrequencyratescaled <- validfrequencyrate %>% mutate(scale_logF=(logF-meanlogF)/sdlogF)
minF<-min(validfrequencyrate$logF)
minF
mean_count_per_day <- validationbinned %>% group_by(userId, time) %>% summarise(countd=n())
mean(mean_count_per_day$countd)

validbinned4 <- validationbinned %>%  left_join(validfrequencyratescaled[,c(1,2,5)], by=c("userId", "time")) %>% mutate(ratings=rating)

###scale the ratings variable to a z distribution
validationsdrating <- sd(validbinned4$ratings)
validmeanrating <- mean(validbinned4$ratings)

validbinned3 <- validbinned4 %>% mutate(rating=(ratings-validmeanrating)/validationsdrating)
##test for mean at zero
validmrating<- mean(validbinned3$rating)
##reduce the size of the table
validationshort <- validbinned3[,c(1,2,3,4,5,6,7,9,12)]

str(validationshort)
###find the RMSE of the model#####the function does not work because there are missing lines/records resulting in an N/A, so RMSE is calculated by formula

predicted_ratings <- validationshort %>% left_join(movie_reg_avgs5, by="movieId") %>% left_join(user_reg_avgs5, by='userId') %>% left_join(bin_reg_avgs5, by="bin") %>% left_join(freq_reg_avgs5, by="scale_logF") %>% left_join (genre_reg_avgs5, by="genres") %>% mutate(pred=mu+b_i+b_u + b_b +b_F+b_g)%>%pull(pred)
#test_set<- validationshort
#str(test_set)
pred_rating <- as.vector(predicted_ratings)
validation_rating <- as.vector(validationshort$rating)
calc<- as_tibble(cbind(pred_rating, validation_rating))%>% mutate(subtract=(validation_rating-pred_rating)^2)
RMSE<- sqrt(mean(calc$subtract, na.rm=TRUE))
RMSE


##check again
findRMSE <- sqrt(mean((validation_rating-pred_rating)^2, na.rm=TRUE))
findRMSE

rmse_results <- add_row(rmse_results, method="validation set", RMSE=findRMSE)



```





```{r, rmse-results-table, echo=FALSE, eval=TRUE, fig.dim=c(7,4)}


kable(rmse_results, caption="RMSE results, all models")

```

**Conclusions**

The models tested yielded the greatest effect from the relationships between ratings and the user,u and the movie, i.  All other factors entered into my model were improvements, but in the linear model, they only contributed small increments toward understanding the tastes and preferences of users.  Factor analysis or other techniques might be better techniques for nonlinear variables such as genres or for changes in taste over time.  Also noted is that to create efficient code, it is important to remember that the database is huge and that pulling out only the variables of interest for a particular process will save on processing speeds and RAM.  Also learned was that despite the allure of complex models with lots of detailed estimations, the RAM available for processing will limit what is possible.

To improve processing speed for some data manipulations and visual presentations, I created an index for the genres.  It would be nice to have that index created formally, so that everyone processing the data base has easy access and understanding of the index created.
The most important lesson learned was the value of scaling the variables, as the best improvements to RMSE were obtained with the scale transformations.  Standardizing the beta estimates also yielded some very good improvements to the values obtained. 



